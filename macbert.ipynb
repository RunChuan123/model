{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer,BertTokenizerFast,AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import random\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score , accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639304f009894ef1981ef9aead1ff50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llama3conda\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\code\\vsc\\Transformerc\\MacBERT\\macbert_end\\model\\models--hfl--chinese-macbert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36dcfed0cf554292b8e840b71ea608bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llama3conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#加载模型\n",
    "model = AutoModel.from_pretrained('hfl/chinese-macbert-base',cache_dir = './model') # \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#冻结模型参数\n",
    "for pa in model.parameters():\n",
    "    pa.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cedf4297494e80816511d19442e7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a635189669448d5a29cc418ffe20778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5ddc956298453ea11c50bbb98649be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset('./exercise_contest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itos and stoi\n",
    "name_to_num = {}\n",
    "num_to_name =  {}\n",
    "count = 0\n",
    "for i in data['train']:\n",
    "    for j in i['meta']['accusation']:\n",
    "        if(j not in name_to_num.keys()):\n",
    "            name_to_num[j]=count\n",
    "            count+=1\n",
    "for key, value in name_to_num.items():\n",
    "    num_to_name[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19276"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 处理数据集不平衡 修改每种罪名数据大概要加载多少条\n",
    "name_per_label_number = {k:0 for k in name_to_num.keys()}\n",
    "for i in data['train']:\n",
    "    for single_name in i['meta']['accusation']:\n",
    "        name_per_label_number[single_name] +=1\n",
    "name_per_label_number\n",
    "sample_number_per_label = {k:0 for k in name_to_num.keys()}\n",
    "for k,v in name_per_label_number.items():\n",
    "    v = v / 7\n",
    "    if v < 30:\n",
    "        sample_number_per_label[k] = 30\n",
    "    else:\n",
    "        number = math.floor(v / 2 ) + 30\n",
    "        sample_number_per_label[k] = number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    def __init__(self,dataset_type:str,tokenizer:BertTokenizerFast=tokenizer):\n",
    "        self.sample_list = []\n",
    "        self.tokenizer = tokenizer\n",
    "        # 加载对应数据集\n",
    "        data_type = data[dataset_type].shuffle(seed = 3)\n",
    "        # 对训练数据进行采样 按照罪名应该采样多少\n",
    "        if data_type == 'train':\n",
    "            current_number_per_label = {k:0 for k in name_to_num.keys()}\n",
    "            for i in data_type:\n",
    "                current_data_i = random.choice(i['meta']['accusation'])\n",
    "                if current_number_per_label[current_data_i] > sample_number_per_label[current_data_i]:\n",
    "                    continue\n",
    "                input_data = str({'fact':i['fact'],'life_imprisonment':i['meta']['term_of_imprisonment']['life_imprisonment'],'death_penalty': i['meta']['term_of_imprisonment']['death_penalty'],'imprisonment': i['meta']['term_of_imprisonment']['imprisonment'],'criminals':i['meta']['criminals'],'relevant_articles':i['meta']['relevant_articles']})\n",
    "                label = list(map(lambda x: name_to_num[x],i['meta']['accusation']))\n",
    "                self.sample_list.append((input_data,label))\n",
    "                current_number_per_label[current_data_i] += 1\n",
    "        else:\n",
    "            # 普通采样\n",
    "            count = 0\n",
    "            for i in data_type:\n",
    "                if len(i['fact']) < 328:\n",
    "                    input_data = str({'fact':i['fact'],'life_imprisonment':i['meta']['term_of_imprisonment']['life_imprisonment'],'death_penalty': i['meta']['term_of_imprisonment']['death_penalty'],'imprisonment': i['meta']['term_of_imprisonment']['imprisonment'],'criminals':i['meta']['criminals'],'relevant_articles':i['meta']['relevant_articles']})\n",
    "                    label = list(map(lambda x: name_to_num[x],i['meta']['accusation']))\n",
    "                    self.sample_list.append((input_data,label))\n",
    "                    count += 1\n",
    "                    if count >7000:\n",
    "                        break\n",
    "    def __getitem__(self,index):\n",
    "        input_data,label = self.sample_list[index]\n",
    "        label_onehot = torch.zeros(len(name_to_num))\n",
    "        for label_i in label:\n",
    "            label_onehot[label_i] = 1\n",
    "        input_tokenized = self.tokenizer(text=input_data,padding='max_length',max_length=512,truncation=True,return_tensors='pt')\n",
    "        return {\n",
    "            'input_data':input_data,\n",
    "            'input_ids':input_tokenized['input_ids'].squeeze(),\n",
    "            'attention_mask':input_tokenized['attention_mask'].squeeze(),\n",
    "            'label':label_onehot\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加了头后的模型\n",
    "class macbert(nn.Module):\n",
    "    def  __init__(self):\n",
    "        super(macbert,self).__init__()\n",
    "        self.bert = model\n",
    "        # self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = classifyner()\n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        output_1 = self.bert(input_ids,attention_mask)\n",
    "        output_2 = output_1['last_hidden_state']\n",
    "        output_2 = output_2.mean(dim=1)\n",
    "        logits = self.classifier(output_2)\n",
    "        return logits\n",
    "class classifyner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifyner,self).__init__()\n",
    "        self.fc_1 = nn.Linear(768, 768)\n",
    "        self.gule = nn.GELU()\n",
    "        self.fc_2 = nn.Linear(768, 202)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.gule(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Mydataset('train',tokenizer)\n",
    "data_train = DataLoader(data_train,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_30628\\2440931227.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mo.load_state_dict(torch.load('model2.pth'))\n",
      "100%|██████████| 5001/5001 [04:47<00:00, 17.39it/s, loss=0.000103]\n",
      "100%|██████████| 5001/5001 [04:48<00:00, 17.34it/s, loss=0.000488]\n",
      "100%|██████████| 5001/5001 [04:48<00:00, 17.32it/s, loss=0.000172]\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "device = torch.device('cuda:0')\n",
    "mo = macbert().to(device)\n",
    "mo.load_state_dict(torch.load('model2.pth'))\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad,mo.parameters()), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    mo.train()\n",
    "    train_tqdm = tqdm(data_train)\n",
    "    for batch in train_tqdm:\n",
    "        input_ids ,attention_mask, label = batch['input_ids'].to(device),batch['attention_mask'].to(device),batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = mo(input_ids,attention_mask)\n",
    "        loss = criterion(output,label)\n",
    "        train_tqdm.set_postfix(loss = loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(mo.state_dict(),f'model_sample{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_30628\\81377433.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_valid.load_state_dict(torch.load('model_sample3.pth'))\n",
      "100%|██████████| 1751/1751 [01:29<00:00, 19.59it/s]\n",
      "100%|██████████| 1751/1751 [01:29<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8730181402656764 0.8825405894396038 0.8598771604056563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llama3conda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 验证模型的准确率召回率和正确率\n",
    "\n",
    "data_valid = Mydataset('validation',tokenizer)\n",
    "data_valid = DataLoader(data_valid,batch_size=4,shuffle=True)\n",
    "\n",
    "model_valid = macbert().to(device)\n",
    "model_valid.load_state_dict(torch.load('model_sample3.pth'))\n",
    "sig = nn.Sigmoid()\n",
    "\n",
    "pred_list = []\n",
    "label_list = []\n",
    "data_tqdm = tqdm(data_valid)\n",
    "\n",
    "model_valid.eval()\n",
    "\n",
    "for t in tqdm(data_tqdm):\n",
    "    input_ids ,attention_mask, label = t['input_ids'].to(device),t['attention_mask'].to(device),t['label'].to(device)\n",
    "    output = model_valid(input_ids,attention_mask)\n",
    "    pred = sig(output)\n",
    "    for i in zip(pred,label):\n",
    "        pred_list.append((i[0]>0.9).to(torch.int))\n",
    "        label_list.append(i[1])\n",
    "pred_list = [i.cpu().detach().numpy() for i in pred_list]\n",
    "label_list = [i.cpu().detach().numpy() for i in label_list]\n",
    "\n",
    "p_score = precision_score(pred_list,label_list,average='samples')\n",
    "r_score = recall_score(pred_list,label_list,average='samples') \n",
    "a_score = accuracy_score(pred_list,label_list)\n",
    "print(p_score,r_score,a_score)\n",
    "# 0.8730181402656764 0.8825405894396038 0.8598771604056563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23804\\3753101824.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_test.load_state_dict(torch.load('model_sample3.pth'))\n"
     ]
    }
   ],
   "source": [
    "# 模型测试输入输出\n",
    "\n",
    "model_test = macbert().to(device)\n",
    "model_test.load_state_dict(torch.load('model_sample3.pth'))\n",
    "\n",
    "sig = nn.Sigmoid()\n",
    "def get_pred(input_data):\n",
    "    model_test.eval()\n",
    "    input_tokenized = tokenizer(text=input_data,padding='max_length',max_length=512,truncation=True,return_tensors='pt')\n",
    "    input_ids = input_tokenized['input_ids'].to(device)\n",
    "    attention_mask = input_tokenized['attention_mask'].to(device)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        output = model_test(input_ids,attention_mask)\n",
    "        \n",
    "        metric = (sig(output) > 0.9).to(torch.int).squeeze().cpu().numpy()\n",
    "        output = [i for i,v in enumerate(metric) if v==1]\n",
    "        for i in output:\n",
    "            pred.append(num_to_name[i])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llama3conda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'非法行医'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = '''\"{'fact': '永嘉县人民检察院指控，2014年4月至2015年9月期间，被告人李某在未取得医生执业资格的情况下，伙同他人在温州市瑞安市汀田街道联盟村、瑞安市塘下等地用便携式b超机先后为张某、刘某、缪某某、雷某某、兰某等人进行非医学需要的胎儿性别鉴定，共7人次以上，并导致他人将胎儿引产。其中，被告人李某在2015年8月25日为缪某某、雷某某进行非医学需要的胎儿性别鉴定时被温州经济技术开发区民政卫生和计划生育局查获，并被罚款人民币1万元。案发后，被告人李某于2015年11月5日于温州市瑞安市汀田街道联盟村盟兴巷26号被民警抓获。', 'life_imprisonment': False, 'death_penalty': False, 'imprisonment': 9, 'criminals': ['李某'], 'relevant_articles': [336]}\"'''\n",
    "get_pred(input)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
